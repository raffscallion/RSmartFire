library(rgdal)
library(rgeos)
library(dplyr)
# Load shapefile
filename <- "2011_perimeters_dd83"
path <- "./InputData/GeoMac"
outpath <- "./InputData/Tranche1"
raw.shapes <- readOGR(path, filename)
# Union to find intersections (will need to fix topology first)
bad.geoms <- gIsValid(raw.shapes, byid=TRUE)
shapes <- raw.shapes[bad.geoms==TRUE,]
# need to recalculate IDs to get code below to work (we need id == rownumber)
n <- length(slot(shapes, 'polygons'))
shapes <- spChFIDs(shapes, as.character(1:n))
# extract the data frame with IDs attached for convenience and add end.date
# in proper date format
d <- shapes@data %>%
mutate(ID = as.numeric(rownames(as(shapes, "data.frame"))),
end.date = as.Date(date_))
# 1. find all names with more than one record
conflicts <- group_by(d, fire_name) %>%
summarise(records = n()) %>%
filter(records > 1) %>%
arrange(fire_name)
# Look name by name and find most recent disjoint set (throw out old intersecting polys)
# the dreaded for loop
master.list <- list()
master.df <- data.frame(ID=character(), first.date=character())
for (f in 1:nrow(conflicts)) {
firename <- conflicts$fire_name[[f]]
set <- shapes[shapes$fire_name == firename,]
# determine all intersections
intersections <- gIntersects(set, byid=TRUE)
# for each intersection - select record with latest date (discard all others)
# also record the first date for later use
#### Step through each row of array - find the id with latest date among each true
### The final list will be the set of ids to discard
# We'll start this as a loop and hopefully improve to array based later
keep.list = list()
first.date.list = list()
for (n in 1:nrow(intersections)) {
# These are the ids that intersect
ids <- names(which(intersections[n,], useNames=TRUE))
# Find the record with the best date
set <- filter(d, ID %in% ids)
keep.df <- arrange(set, desc(end.date)) %>%
slice(1)
# add the ID to the keep list
keep.list <- c(keep.list, keep.df$ID)
# Find the record with the first date
first <- arrange(set, end.date) %>%
slice(1)
first.date.list <- c(first.date.list, as.character(first$end.date))
}
# merge first date and ids to a dataframe
date.id <- data.frame(ID=as.numeric(unlist(keep.list)), first.date=unlist(first.date.list), stringsAsFactors=FALSE)
# Get the unique values from the keep.list and add to master list
date.id <- distinct(date.id)
master.df <- rbind(master.df, date.id)
}
# add the single record files back to the master list
single <- group_by(d, fire_name) %>%
mutate(records = n(),
first.date = as.character(end.date)) %>%
filter(records == 1)
single <- select(as.data.frame(single), ID, first.date)
master.df <- rbind(master.df, single)
# Make sure final master list is unique, convert to vector, then subset the shapefile
master.df <- group_by(master.df, ID) %>%
summarise(first.date = min(first.date)) %>%
arrange(ID)
master.vector <- unlist(master.df$ID)
shapes.final <- shapes[master.vector,]
# Perhaps will also add common fields here
# Add ID as a key for joining
shapes.final$ID <- as.character(row.names(shapes.final))
# Add the first.date and everything else in master.df (see http://stackoverflow.com/questions/3650636/how-to-attach-a-simple-data-frame-to-a-spatialpolygondataframe-in-r)
shapes.final@data <- data.frame(shapes.final@data, master.df[match(shapes.final@data[,'ID'], master.df[,'ID']),])
# Recalculate IDs (shouldn't need to do this)
n <- length(slot(shapes.final, 'polygons'))
shapes.final <- spChFIDs(shapes.final, as.character(1:n))
# Put into the central projection (CONUS Albers equal area)
shapes.proj <- spTransform(shapes.final, CRS("+init=epsg:5070"))
# Put in the standard sf fields
shapes.proj@data <- mutate(shapes.proj@data,
sf_area = acres,
sf_start = as.Date(first.date),
sf_end = as.Date(date_, format='%Y/%m/%d'),
sf_type = 'WF',
sf_name = fire_name,
sf_source = 'GeoMac',
sf_id = as.character(ID))
# Export to shapefile
writeOGR(shapes.proj, outpath, 'GeoMacProcessed', 'ESRI Shapefile')
###  DevelopPrimaryPolygons.R
#
# First attempt at creating primary polygons from Tranche 1 data sets
#
#   1. Load preprocessed data from shapefile (or directly from R potentially)
#   2. Look for intersecting polygons
#   3. Create a crosswalk list for later use
#   4. Create a best-guess polygon (based on hierarchy?)
#   5. Write out to a constant format for later processing
#
### Parameters to change
# The working directory
setwd("C:/Users/sraffuse/Google Drive/Work/R Code/SF3/SF_Algorithms")
# The list of reprocessed Tranche1 datasets (in same path)
# The earliest listed dataset takes precedence over subsequent data sets
inputs <- list('MN_WF', 'MN_RX', 'GeoMacProcessed')
# The location of the input Tranche1 datasets
inpath <- "./InputData/Tranche1"
outpath <- "./FinalData/Tranche1"
outname <- 'Tranche1PolygonsTest'
# Required packages
library(rgdal)
library(rgeos)
library(dplyr)
# Load shapefiles to a list
datasets <- lapply(inputs, function(x) {readOGR(dsn=inpath, layer=x, stringsAsFactors=FALSE)})
# Look for intersecting polygons across all layers two at a time (Magic!)
combos <- combn(datasets, 2, function(x) gIntersects(x[[1]],x[[2]], byid=TRUE), simplify=FALSE)
ints <- lapply(combos, which, arr.ind=TRUE)
# Now we have the full list of intersections, what do we do?
# Throw out any polys that intersect with the first listed dataset,
# then throw out remaining polys that intersect with the next listed dataset.
# This method ignores time.
# Here are the different combinations of i and j
n.combos <- length(ints)
n.datasets <- length(datasets)
x <- 1
i <- 1
j <- 2
pairs <- matrix(ncol=2, nrow=n.combos)
while (x <= n.combos) {
pairs[x,1] <- i
pairs[x,2] <- j
j <- j+1
if (j > n.datasets) {
i <- i+1
j <- i+1
}
x <- x+1
}
# Now walk through the list of intersections and build the list to discard
n.intersects <- length(unlist(ints))/2
# make a structure to store the list (dataset, dup.id)
toss <- setNames(data.frame(matrix(ncol = 2, nrow = n.intersects)),c("dataset", "dup.id"))
i <- 1
for (x in 1:n.combos) {
dups <- ints[[x]][,1]
dup.polys <- length(dups)
for (y in 1:dup.polys) {
toss$dataset[i] <- pairs[x,2]
toss$dup.id[i] <- dups[[y]]
i <- i+1
}
}
# the distinct list of conflicting polygons to toss
toss.distinct <- distinct(toss)
# This method does not associate or count data sets that conflicted, which should be improved.
# For now, we save out the variable 'ints,' which contains all of the intersections
# and can later be used to reconstruct associations.
save(ints, file = "tranche1intersections.RData")
# Remove conflicting polygons from each dataset
ds.index <- seq(1:n.datasets)
removeConflicts <- function(x, y) {
toss.1 <- filter(toss.distinct, dataset == y) %>%
mutate(sf_id = as.character(dup.id))
to.keep <- anti_join(x@data, toss.1, by='sf_id') %>%
arrange(sf_id)
to.keep.v <- unlist(to.keep$sf_id)
no.dup <- x[x$sf_id %in% to.keep.v,]
}
datasets.no.dups <- mapply(removeConflicts, datasets, ds.index)
# Delete all non-sf fieldnames
cleanFields <- function(x) {
x@data <- (select(x@data, starts_with('sf_')))
return(x)
}
datasets.no.dups <- mapply(cleanFields, datasets.no.dups)
# Merge together and write to shapefile
# Recalculate merged sf_id values (from sfUtils.R)
source('./Code/sfUtils.R')
newIDs<-mapply(make_sf_id, datasets.no.dups, ds.index, 1)
merged <- do.call(rbind, newIDs)
# change the sf_id to the rownames
merged@data$sf_id <- row.names(merged)
writeOGR(merged, outpath, outname, 'ESRI Shapefile')
###  DevelopPrimaryPolygons.R
#
# First attempt at creating primary polygons from Tranche 1 data sets
#
#   1. Load preprocessed data from shapefile (or directly from R potentially)
#   2. Look for intersecting polygons
#   3. Create a crosswalk list for later use
#   4. Create a best-guess polygon (based on hierarchy?)
#   5. Write out to a constant format for later processing
#
### Parameters to change
# The working directory
setwd("C:/Users/sraffuse/Google Drive/Work/R Code/SF3/SF_Algorithms")
# The list of reprocessed Tranche1 datasets (in same path)
# The earliest listed dataset takes precedence over subsequent data sets
inputs <- list('MN_WF', 'MN_RX', 'GeoMacProcessed')
# The location of the input Tranche1 datasets
inpath <- "./InputData/Tranche1"
outpath <- "./FinalData/Tranche1"
outname <- 'Tranche1Polygons'
# Required packages
library(rgdal)
library(rgeos)
library(dplyr)
# Load shapefiles to a list
datasets <- lapply(inputs, function(x) {readOGR(dsn=inpath, layer=x, stringsAsFactors=FALSE)})
# Look for intersecting polygons across all layers two at a time (Magic!)
combos <- combn(datasets, 2, function(x) gIntersects(x[[1]],x[[2]], byid=TRUE), simplify=FALSE)
ints <- lapply(combos, which, arr.ind=TRUE)
# Now we have the full list of intersections, what do we do?
# Throw out any polys that intersect with the first listed dataset,
# then throw out remaining polys that intersect with the next listed dataset.
# This method ignores time.
# Here are the different combinations of i and j
n.combos <- length(ints)
n.datasets <- length(datasets)
x <- 1
i <- 1
j <- 2
pairs <- matrix(ncol=2, nrow=n.combos)
while (x <= n.combos) {
pairs[x,1] <- i
pairs[x,2] <- j
j <- j+1
if (j > n.datasets) {
i <- i+1
j <- i+1
}
x <- x+1
}
# Now walk through the list of intersections and build the list to discard
n.intersects <- length(unlist(ints))/2
# make a structure to store the list (dataset, dup.id)
toss <- setNames(data.frame(matrix(ncol = 2, nrow = n.intersects)),c("dataset", "dup.id"))
i <- 1
for (x in 1:n.combos) {
dups <- ints[[x]][,1]
dup.polys <- length(dups)
for (y in 1:dup.polys) {
toss$dataset[i] <- pairs[x,2]
toss$dup.id[i] <- dups[[y]]
i <- i+1
}
}
# the distinct list of conflicting polygons to toss
toss.distinct <- distinct(toss)
# This method does not associate or count data sets that conflicted, which should be improved.
# For now, we save out the variable 'ints,' which contains all of the intersections
# and can later be used to reconstruct associations.
save(ints, file = "tranche1intersections.RData")
# Remove conflicting polygons from each dataset
ds.index <- seq(1:n.datasets)
removeConflicts <- function(x, y) {
toss.1 <- filter(toss.distinct, dataset == y) %>%
mutate(sf_id = as.character(dup.id))
to.keep <- anti_join(x@data, toss.1, by='sf_id') %>%
arrange(sf_id)
to.keep.v <- unlist(to.keep$sf_id)
no.dup <- x[x$sf_id %in% to.keep.v,]
}
datasets.no.dups <- mapply(removeConflicts, datasets, ds.index)
# Delete all non-sf fieldnames
cleanFields <- function(x) {
x@data <- (select(x@data, starts_with('sf_')))
return(x)
}
datasets.no.dups <- mapply(cleanFields, datasets.no.dups)
# Merge together and write to shapefile
# Recalculate merged sf_id values (from sfUtils.R)
source('./Code/sfUtils.R')
newIDs<-mapply(make_sf_id, datasets.no.dups, ds.index, 1)
merged <- do.call(rbind, newIDs)
# change the sf_id to the rownames
merged@data$sf_id <- row.names(merged)
writeOGR(merged, outpath, outname, 'ESRI Shapefile')
###  DevelopTranche2Polygons.R
#
# First attempt at creating final combined polygons from Tranche 2 data sets
#
#   1. Load preprocessed data from shapefile (or directly from R potentially)
#   2. Look for intersecting polygons
#   3. Create a crosswalk list for later use
#   4. For each intersection, determine if they are within date range
#   5. Append all information from lower rank data to a blob in high rank data
#   6. Write out to a constant format for later processing
#
### Parameters to change
# The working directory
setwd("C:/Users/sraffuse/Google Drive/Work/R Code/SF3/SF_Algorithms")
# The list of reprocessed Tranche1 datasets (in same path)
# The earliest listed dataset takes precedence over subsequent data sets
inputs <- list('MN_DNR_WF_Tranche2', 'MN_FWS_Tranche2', 'NASF_Tranche2', 'FACTS_Tranche2')
# The location of the input Tranche1 datasets
inpath <- "./FinalData/Tranche2/"
outpath <- "./FinalData/Tranche2"
outname <- 'Tranche2PolygonsTest'
# Required packages
library(rgdal)
library(rgeos)
library(dplyr)
# Load shapefiles to a list (previously saved as RDS files)
datasets <- lapply(inputs, function(x) {readRDS(paste0(inpath, x, '.RDS'))})
# Look for intersecting polygons across all layers two at a time (Magic!)
combos <- combn(datasets, 2, function(x) gIntersects(x[[1]],x[[2]], byid=TRUE), simplify=FALSE)
ints <- lapply(combos, which, arr.ind=TRUE)
# the ints variable contains the pairs that intersect, but need to parse it to get
# the specific data sets and ids for each intersect. That is all the mess below
# Here are the different combinations of i and j
n.combos <- length(ints)
n.datasets <- length(datasets)
x <- 1
i <- 1
j <- 2
pairs <- matrix(ncol=2, nrow=n.combos)
while (x <= n.combos) {
pairs[x,1] <- i
pairs[x,2] <- j
j <- j+1
if (j > n.datasets) {
i <- i+1
j <- i+1
}
x <- x+1
}
# Now walk through the list of intersections and build the list to discard
n.intersects <- length(unlist(ints))/2
# make a structure to store the list (ds1, dup.id1 ,ds2, dup.id2)
candidates <- setNames(data.frame(matrix(ncol = 4, nrow = n.intersects)),c("ds1", "dup.id1", "ds2", "dup.id2"))
i <- 1
for (x in 1:n.combos) {
dups.ds1 <- ints[[x]][,2]
dups.ds2 <- ints[[x]][,1]
dup.polys <- length(dups.ds1)
if (dup.polys==0) next
for (y in 1:dup.polys) {
candidates$ds1[i] <- pairs[x,1]
candidates$dup.id1[i] <- dups.ds1[[y]]
candidates$ds2[i] <- pairs[x,2]
candidates$dup.id2[i] <- dups.ds2[[y]]
i <- i+1
}
}
# Now for each candidate intersection, check to see if they intersect in time (or are close?)
checkDates <- function(ds1, id1, ds2, id2) {
start1 <- datasets[[ds1]][id1,]$sf_start
end1 <- datasets[[ds1]][id1,]$sf_end
start2 <- datasets[[ds2]][id2,]$sf_start
end2 <- datasets[[ds2]][id2,]$sf_end
if ((abs(start1 - start2) < 3) | (abs(end1 - end2) < 3)) {
return(TRUE)
} else return(FALSE)
}
candidates$overlap <- mapply(checkDates, candidates$ds1, candidates$dup.id1, candidates$ds2, candidates$dup.id2)
# Now, for all candidates where overlap is true, remove the secondary data set
toss <- filter(candidates, overlap==TRUE)
# Remove conflicting polygons from each dataset
ds.index <- seq(1:n.datasets)
removeConflicts <- function(x, y) {
toss.1 <- filter(toss, ds2 == y) %>%
mutate(sf_id = as.character(dup.id2))
to.keep <- anti_join(x@data, toss.1, by='sf_id') %>%
arrange(sf_id)
to.keep.v <- unlist(to.keep$sf_id)
no.dup <- x[x$sf_id %in% to.keep.v,]
}
datasets.no.dups <- mapply(removeConflicts, datasets, ds.index)
# Delete all non-sf fieldnames
cleanFields <- function(x) {
x@data <- (select(x@data, starts_with('sf_')))
return(x)
}
datasets.no.dups <- mapply(cleanFields, datasets.no.dups)
# Merge together and write to shapefile
source('./Code/sfUtils.R')
newIDs<-mapply(make_sf_id, datasets.no.dups, ds.index, 2)
merged <- do.call(rbind, newIDs)
# change the sf_id to the rownames
merged@data$sf_id <- as.character(row.names(merged))
writeOGR(merged, outpath, outname, 'ESRI Shapefile')
###  DevelopTranche2Polygons.R
#
# First attempt at creating final combined polygons from Tranche 2 data sets
#
#   1. Load preprocessed data from shapefile (or directly from R potentially)
#   2. Look for intersecting polygons
#   3. Create a crosswalk list for later use
#   4. For each intersection, determine if they are within date range
#   5. Append all information from lower rank data to a blob in high rank data
#   6. Write out to a constant format for later processing
#
### Parameters to change
# The working directory
setwd("C:/Users/sraffuse/Google Drive/Work/R Code/SF3/SF_Algorithms")
# The list of reprocessed Tranche1 datasets (in same path)
# The earliest listed dataset takes precedence over subsequent data sets
inputs <- list('MN_DNR_WF_Tranche2', 'MN_FWS_Tranche2', 'NASF_Tranche2', 'FACTS_Tranche2')
# The location of the input Tranche1 datasets
inpath <- "./FinalData/Tranche2/"
outpath <- "./FinalData/Tranche2"
outname <- 'Tranche2Polygons'
# Required packages
library(rgdal)
library(rgeos)
library(dplyr)
# Load shapefiles to a list (previously saved as RDS files)
datasets <- lapply(inputs, function(x) {readRDS(paste0(inpath, x, '.RDS'))})
# Look for intersecting polygons across all layers two at a time (Magic!)
combos <- combn(datasets, 2, function(x) gIntersects(x[[1]],x[[2]], byid=TRUE), simplify=FALSE)
ints <- lapply(combos, which, arr.ind=TRUE)
# the ints variable contains the pairs that intersect, but need to parse it to get
# the specific data sets and ids for each intersect. That is all the mess below
# Here are the different combinations of i and j
n.combos <- length(ints)
n.datasets <- length(datasets)
x <- 1
i <- 1
j <- 2
pairs <- matrix(ncol=2, nrow=n.combos)
while (x <= n.combos) {
pairs[x,1] <- i
pairs[x,2] <- j
j <- j+1
if (j > n.datasets) {
i <- i+1
j <- i+1
}
x <- x+1
}
# Now walk through the list of intersections and build the list to discard
n.intersects <- length(unlist(ints))/2
# make a structure to store the list (ds1, dup.id1 ,ds2, dup.id2)
candidates <- setNames(data.frame(matrix(ncol = 4, nrow = n.intersects)),c("ds1", "dup.id1", "ds2", "dup.id2"))
i <- 1
for (x in 1:n.combos) {
dups.ds1 <- ints[[x]][,2]
dups.ds2 <- ints[[x]][,1]
dup.polys <- length(dups.ds1)
if (dup.polys==0) next
for (y in 1:dup.polys) {
candidates$ds1[i] <- pairs[x,1]
candidates$dup.id1[i] <- dups.ds1[[y]]
candidates$ds2[i] <- pairs[x,2]
candidates$dup.id2[i] <- dups.ds2[[y]]
i <- i+1
}
}
# Now for each candidate intersection, check to see if they intersect in time (or are close?)
checkDates <- function(ds1, id1, ds2, id2) {
start1 <- datasets[[ds1]][id1,]$sf_start
end1 <- datasets[[ds1]][id1,]$sf_end
start2 <- datasets[[ds2]][id2,]$sf_start
end2 <- datasets[[ds2]][id2,]$sf_end
if ((abs(start1 - start2) < 3) | (abs(end1 - end2) < 3)) {
return(TRUE)
} else return(FALSE)
}
candidates$overlap <- mapply(checkDates, candidates$ds1, candidates$dup.id1, candidates$ds2, candidates$dup.id2)
# Now, for all candidates where overlap is true, remove the secondary data set
toss <- filter(candidates, overlap==TRUE)
# Remove conflicting polygons from each dataset
ds.index <- seq(1:n.datasets)
removeConflicts <- function(x, y) {
toss.1 <- filter(toss, ds2 == y) %>%
mutate(sf_id = as.character(dup.id2))
to.keep <- anti_join(x@data, toss.1, by='sf_id') %>%
arrange(sf_id)
to.keep.v <- unlist(to.keep$sf_id)
no.dup <- x[x$sf_id %in% to.keep.v,]
}
datasets.no.dups <- mapply(removeConflicts, datasets, ds.index)
# Delete all non-sf fieldnames
cleanFields <- function(x) {
x@data <- (select(x@data, starts_with('sf_')))
return(x)
}
datasets.no.dups <- mapply(cleanFields, datasets.no.dups)
# Merge together and write to shapefile
source('./Code/sfUtils.R')
newIDs<-mapply(make_sf_id, datasets.no.dups, ds.index, 2)
merged <- do.call(rbind, newIDs)
# change the sf_id to the rownames
merged@data$sf_id <- as.character(row.names(merged))
writeOGR(merged, outpath, outname, 'ESRI Shapefile')
### MergeTranche1And2.R
#  Combines tranche 1 and 2 into a single data set for further processing
#
#
library(rgdal)
library(maptools)
setwd("C:/Users/sraffuse/Google Drive/Work/R Code/SF3/SF_Algorithms")
outpath <- "./FinalData/Merged"
outname <- 'Tranches1and2'
# Get tranche1
t1.polys <- readOGR(dsn='./FinalData/Tranche1', layer='Tranche1Polygons', stringsAsFactors=FALSE)
# Get tranche2
t2.polys <- readOGR(dsn='./FinalData/Tranche2', layer='Tranche2Polygons', stringsAsFactors=FALSE)
# populate spids with sf_id
t1.polys <- spChFIDs(t1.polys, t1.polys$sf_id)
t2.polys <- spChFIDs(t2.polys, t2.polys$sf_id)
# Merge (might use rbind, but columns are out of order, ok?)
# Also, IDs must be unique - go back and add T1_ and T2_ to the rownames
merged.polys <- spRbind(t1.polys, t2.polys)
writeOGR(merged.polys, outpath, outname, 'ESRI Shapefile')
