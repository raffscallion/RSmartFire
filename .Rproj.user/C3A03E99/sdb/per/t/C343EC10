{
    "contents" : "######    ProcessTranche2Dataset.R\n#Prepare Tranche 2 Dataset\n##\n#\n#  Assign (associate) as many hotspots/point fire locations/reports as\n#    possible to the known fire perimeters (\"Primary Polygons\") without \n#    clumping the hotspots first.\n#  Only then create via clumping or other methods a set of final/current \n#    \"Derived Polygons\" for fires that do not have a \"Primary Polygon\".\n#  Assign (associate) the individual hotspots / point reports to these \n#    \"Derived Polygons\".\n#\n#  This particular script simply splits the input point data into two shapefiles.\n#   One that is overlaps or is nearby primary polygons and one that is not.\n#  \n\n# Parameters\n# The working directory\nsetwd(\"C:/Users/Sean/Google Drive/Work/R Code/SF3/SF_Algorithms\")\n# Distance in meters considered associated with the primary poly (should relate to spatial uncertainty)\nwithin.distance <- 500   \n\ninput.name <- 'FACTS'  # A friendly name for metadata and the output name\n#'MN_DNR_WF'\n#'NASF'\n#'FACTS'\n\nswitch(input.name,\n       MN_DNR_WF = {\n         columns <- c(NA, NA, NA, NA, NA,NA, NA, NA, NA, NA, NA, NA, NA, 'numeric', 'numeric')\n         coord.fields <- c(\"longitude\", \"latitude\")\n         input.file <- './InputData/MN/2011 MN DNR Wildfires.csv'\n         output.path <- './FinalData/Tranche2'},\n       NASF = {\n         columns <- c(NA, NA, NA, NA, NA,NA, NA, NA, NA, NA, NA, NA, NA, NA, 'numeric', 'numeric',\n                      NA, NA, NA, NA, NA,NA, NA, NA, NA, NA, NA, NA, NA)\n         coord.fields <- c(\"Longitude\", \"Latitude\")\n         input.file <- './InputData/NASF/2011_NASF_clean.csv'\n         output.path <- './FinalData/Tranche2'},\n       FACTS = {\n         columns <- c(NA, NA, NA, NA, NA, NA, NA, NA, 'numeric', 'numeric',\n                      NA, NA, NA, NA, NA, NA, NA)\n         coord.fields <- c(\"LONGITUDE\", \"LATITUDE\")\n         input.file <- './InputData/FACTS/20120410_FY11_FireTreatments.csv'\n         output.path <- './FinalData/Tranche2'}\n       )\n\n# Need to ensure lat and lon fields are numeric - default is character\n\n# Simplification tolerance (in meters)\ntolerance <- 20\n\n# Required packages\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(dplyr)\n\nm2.per.acre <- 4046.856\n\n# Get points\npoints.csv <- read.csv(input.file, stringsAsFactors=FALSE, colClasses=columns)\n\n# Make MN only for testing (special for NASF)\npoints.csv <- filter(points.csv, STATE=='Minnesota')\n\n# Promote to spatial\npoints <- SpatialPointsDataFrame(coords = points.csv[, coord.fields], data = points.csv)\n# Add WGS-84 coordinate system (assumed if we simply have lat/lon csv data)\nproj4string(points) <- \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n\n# Put into the central projection (CONUS Albers equal area)\npoints <- spTransform(points, CRS(\"+init=epsg:5070\"))\n\n# Add the SF official fields\n# ID, startdate, enddate, type, source\npoints@data <- switch(input.name, \n                      MN_DNR_WF = mutate(points@data, \n                                         sf_id = rownames(points@data),\n                                         sf_area = total_acres,\n                                         sf_start = as.Date(strptime(discovery_date, format=\"%m/%d/%Y\")),\n                                         sf_end = as.Date(strptime(fire_out_date, format=\"%m/%d/%Y\")),\n                                         sf_type = 'WF',\n                                         sf_name = fire_number,\n                                         sf_source = input.name),\n                      NASF = mutate(points@data, \n                                         sf_id = rownames(points@data),\n                                         sf_area = Final_Fire_Acre_Quantity,\n                                         sf_start = as.Date(strptime(Fire_Discovery_Date, format=\"%m/%d/%Y\")),\n                                         sf_end = as.Date(strptime(EndDate, format=\"%m/%d/%Y\")),\n                                         sf_type = 'WF',\n                                         sf_name = Incident_Name,\n                                         sf_source = input.name),\n                      FACTS = mutate(points@data, \n                                    sf_id = rownames(points@data),\n                                    sf_area = Accomplished.Acres,\n                                    sf_start = as.Date(strptime(Accomplished.Date, format=\"%Y-%m-%d\")),\n                                    sf_end = as.Date(strptime(Completed.Date, format=\"%Y-%m-%d\")),\n                                    sf_type = 'RX',\n                                    sf_name = NA,\n                                    sf_source = input.name)\n)\n\n\n# Get primary polygons\np.polys <- readOGR(dsn='./FinalData/Tranche1', layer='Tranche1Polygons', stringsAsFactors=FALSE)\n\n# Calculate distance to nearest primary polygons and write out nearest ID plus distance\n# There are a couple ways to do this:\n# The slow naive approach calculates the distance matrix between all pairs.  This will not\n# scale well at all:\n##dist <- gDistance(points, p.polys, byid=TRUE)\n# The \"correct\" approach would be a function that returns the nearest object in dataset B\n# for each object in dataset A.  However, this does not yet exist.  See https://stat.ethz.ch/pipermail/r-sig-geo/2013-April/018140.html\n# The best leftover approach is to specify a distance of interest, create new buffered\n# polygons that include that distance, then intersect those with the points.  This does not\n# give distance, just a binary in or out relative to the polygons.\n\n# Running gSimplify before all of this to improve performance.\np.polys <- gSimplify(p.polys, tolerance, topologyPreserve=TRUE)\n\n# buffer the polys\npoly.buffered <- gBuffer(p.polys, byid=TRUE, width=within.distance)\n\n# intersect\nints <- gIntersects(points, poly.buffered, byid=TRUE)\nints.collapse <- apply(ints, 2, function(x) {sum(x)})\n\n# Split into two outputs, those within threshold and those without\nunmatched <- points[ints.collapse == 0,]\nmatched <- points[ints.collapse > 0,]\n\n# We should capture these matched IDs for later processing or joining with final data\nwriteOGR(matched, paste(output.path,'matched',sep='/'), paste0(input.name, '_matched') , 'ESRI Shapefile')\n\n# Turn unmatched data into polygons, using the area field to determine size\nbuffer.sizes <- ((unmatched$sf_area * m2.per.acre)/pi)^0.5\nunmatched.polys <- gBuffer(unmatched, byid=TRUE, width=buffer.sizes)\n\n# Save the unmatched as Tranche 2 results\nwriteOGR(unmatched.polys, output.path, paste0(input.name, '_Tranche2') , 'ESRI Shapefile')\n# Just save as R data to avoid truncation? (save as RDS to load as different name)\nsaveRDS(unmatched.polys, file = paste0(output.path, '/', input.name, '_Tranche2.RDS'))\n",
    "created" : 1430922512717.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2222443006",
    "id" : "C343EC10",
    "lastKnownWriteTime" : 1431109843,
    "path" : "C:/Users/Sean/Google Drive/Work/R Code/SF3/SF_Algorithms/Code/ProcessTranche2Dataset.R",
    "project_path" : "Code/ProcessTranche2Dataset.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}